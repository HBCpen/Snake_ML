# 報酬設計について
## 基本方針

AIの最終目標は「勝利すること」です。しかし、「勝利」という報酬だけでは、そこに至るまでの中間的な行動（餌を食べる、危険を避けるなど）を学習するのが非常に困難です（これを**スパース報酬問題**と言います）。

そこで、最終目標に繋がるような**「望ましい行動」に対して、こまめに小さな報酬（または罰）を与える**ことが学習を成功させる鍵となります。

---

## 具体的な報酬設計案

### 1. 基本的な報酬 (必須)

これらはゲームの決着に直接関わる、最も重要な報酬です。

*   **勝利した時: `+1.0`**
    *   相手が壁、マップの端、自分、または相手自身の体に衝突して敗北した場合です。これはAIが目指すべき最大の報酬です。
*   **敗北した時: `-1.0`**
    *   自分が衝突して敗北した場合です。AIが最も避けるべき行動であり、強い負の報酬を与えます。
*   **引き分けの時: `0.0` or `-0.1`**
    *   両者が同時に衝突した場合など。引き分けを避けて積極的に勝ちに行くAIにしたい場合は、わずかな負の報酬を与えるのも有効です。

### 2. 学習を促進する中間報酬 (強く推奨)

上記の基本的な報酬だけでは学習が非常に遅いため、以下の「ヒント」となる報酬を追加します。

*   **餌を食べた時: `+0.1` ～ `+0.3`**
    *   **理由:** 勝利に繋がる重要な中間目標です。体を長くすることで、相手を囲い込むなど戦略的な優位を得られます。この報酬がないと、AIは餌を無視してただ生き残るだけの消極的な戦略をとる可能性があります。
    *   **注意:** この報酬が大きすぎると、勝利 (`+1.0`) を無視してでも餌を取りに行き、結果的に負けてしまうリスクがあります。

*   **1ステップ進むごと (生存): `-0.001` (小さな罰)**
    *   **理由:** これは「時間経過ペナルティ」です。この小さな罰により、AIは無意味に時間を過ごす（同じ場所をぐるぐる回るなど）ことを避け、より早く目的（餌や勝利）を達成しようとします。つまり、効率的な動きを学習します。
    *   **代替案:** `+0.001` のような「生存ボーナス」を与える方法もありますが、その場合はAIが何もしないで長生きするだけの「引きこもりAI」になるリスクがあるため、時間経過ペナルティの方が攻撃的なAIを育てやすい傾向にあります。

*   **餌への距離に応じた報酬 (Reward Shaping)**
    *   **計算方法:** `(前のステップでの最寄りの餌との距離 - 現在のステップでの最寄りの餌との距離) * 定数`
    *   **理由:** 餌に近づく行動を直接的に評価するため、学習が劇的に高速化します。「餌はあっちだ」とAIに教えてあげるようなものです。
    *   **注意:** 壁の向こう側にある餌にまっすぐ向かって自滅するなど、短絡的な行動を誘発する可能性もあります。他の報酬とのバランスが重要です。

### 3. 対戦ゲームならではの高度な報酬 (オプション)

より人間らしい、あるいは人間を超える戦略的なAIを目指すなら、以下の報酬も有効です。

*   **相手より体が長いことへの報酬**
    *   **計算方法:** `(自分の体の長さ - 相手の体の長さ) * 定数` を毎ステップ与える。
    *   **理由:** 単に餌を食べるだけでなく、相手との「差」を意識させることができます。これにより、自分が優位な時は守りに入り、不利な時は積極的に餌を狙うといった、状況に応じた戦略を学習する可能性があります。

*   **相手の行動範囲を狭めたことへの報酬**
    *   **計算方法:** 相手の頭の周囲8マスのうち、移動不可能なマス（壁、自分や相手の体）の数を数え、その数が増えたら報酬を与える。
    *   **理由:** 相手を追い詰める、進路を妨害するといった「攻撃的な動き」を直接評価できます。これにより、非常に強力で戦略的なAIが生まれる可能性があります。実装は少し複雑になります。

---

# まとめとおすすめの設計プラン

報酬の値をどうバランスさせるかが成功の鍵です。

### プランA: シンプルで堅実な設計 (まず試すべき)
*   **勝利:** `+1.0`
*   **敗北:** `-1.0`
*   **餌を食べた:** `+0.1`
*   **1ステップごと:** `-0.001` (時間経過ペナルティ)

**特徴:** 基本的でバランスが取れており、多くのケースでうまく機能します。まずこの設定から始めて、AIの挙動を見ながら調整するのが良いでしょう。

### プランB: 学習効率を重視した設計
*   プランAの要素に加えて...
*   **餌への距離変化報酬:** `(前の距離 - 現在の距離) * 0.005`

**特徴:** 学習初期のAIが「何をすれば良いか」を素早く掴むのに役立ちます。学習が停滞した場合に試す価値があります。

### プランC: 対戦特化の高度な設計
*   プランAまたはBの要素に加えて...
*   **相手との長さの差:** `(自分の長さ - 相手の長さ) * 0.002` を毎ステップ
*   **相手の行動妨害:** 相手の周囲の障害物が増えたら `+0.05`

**特徴:** 最強のAIを目指すための設計です。相手の存在を強く意識した、非常に戦略的な行動を学習することが期待できます。

これらの報酬設計を参考に、あなたのAIがどのような戦略を学習するかを観察し、微調整を繰り返していくことが、強力なAIを育てる最良の方法です。